{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqdutPqTBr1KiLCozsdFVx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Logistic_regression/blob/main/Logistic_Regression_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vYcQsed0Cl1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
        "a scenario where logistic regression would be more appropriate.**"
      ],
      "metadata": {
        "id": "93I_a_Ix0osY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear regression is used when the dependent variable is continuous and nature of the regression line is linear. It models data using continuous numeric value. On the other hand, logistic regression is used when the dependent variable is binary in nature."
      ],
      "metadata": {
        "id": "D7E7xtp7109h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear regression and logistic regression are both methods for modeling relationships between variables. They are both used to build statistical models but perform different tasks. Linear regression is used to model linear relationships, while logistic regression is used to model binary outcomes (i.e. whether or not an event happened)."
      ],
      "metadata": {
        "id": "i5qzdoh73hqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Linear regression** is used to solve the regression problems. It is used to model linear relationships. This means that it can be used to predict a continuous outcome (i.e. numerical outcome) based on one or more independent variables. Linear regression is the most basic type of regression and is very common in statistical modeling. Linear regression models are of different forms such as simple linear regression and multiple linear regression.\n",
        " - A simple linear regression model is used to model linear relationships between a dependent variable and one independent variable. Multiple linear regression is used to model linear relationships between a dependent variable and two or more independent variables. The formula for a simple linear regression model is:  y = β0 + βx\n",
        " -  y is the dependent variable, x is the independent variable, β0 is the intercept and β is the slope. \n",
        "\n"
      ],
      "metadata": {
        "id": "KZReTFSL3xCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Logistic regression** is used to solve the classification problems. It is used to model binary outcomes (i.e. whether or not an event happened). This means that it can be used to predict whether or not an event will happen (i.e. yes/no, true/false) based on one or more independent variables. In addition, logistic regression can also be used to model multiclass classification problems. The logistic regression used for multi-class classification is also called multinomial logistic regression."
      ],
      "metadata": {
        "id": "GyiP7LKO4YLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Some of the real-world examples where logistic regression models can be used are:*\n",
        "\n",
        "- Predict whether or not a customer will default on a loan\n",
        "- Predict whether or not a patient will have a heart attack\n",
        "- Predict whether or not an email is a spam\n",
        "- Predict whether or not a student will pass/fail an exam"
      ],
      "metadata": {
        "id": "MFhy1XZF4ipX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfbjRtH80wY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWwPH5cM0x9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the cost function used in logistic regression, and how is it optimized?**"
      ],
      "metadata": {
        "id": "NrvyQIRg0yMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- he cost function used in logistic regression is also known as the cross-entropy or the log loss. The cost function is defined as:\n",
        "\n",
        "-log(hθ(x)) if y = 1\n",
        "\n",
        "-log(1−hθ(x)) if y = 0 \n",
        "- where hθ(x) is the hypothesis function and y is the true label."
      ],
      "metadata": {
        "id": "58gwK4J-48Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - logistic regression to solve classification problems where the outcome is a discrete variable. Usually, we use it to solve binary classification problems. As the name suggests, binary classification problems have two possible outputs.\n",
        "\n",
        "- We utilize the sigmoid function (or logistic function) to map input values from a wide range into a limited interval. Mathematically, the sigmoid function is:\n",
        "\n",
        "  \\[y = g(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z}\\]\n",
        "\n",
        "This formula represents the probability of observing the output y = 1 of a Bernoulli random variable. This variable is either 1 or 0 (y \\in {0,1})."
      ],
      "metadata": {
        "id": "1jqo0VGQ5W_6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r3wFjdrM03f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StHb8knt04KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.**"
      ],
      "metadata": {
        "id": "71kY-xCY04V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Regularization is an important technique used in machine learning to prevent overfitting. In logistic regression, regularization is used to add a penalty term to the cost function that the model is trying to minimize. The penalty term is proportional to the magnitude of the coefficients of the model. The purpose of this penalty term is to reduce the magnitude of the coefficients and thus reduce the complexity of the model."
      ],
      "metadata": {
        "id": "wnMVXeiH50HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Regularization is an important technique used in machine learning to prevent overfitting. In logistic regression, regularization is used to add a penalty term to the cost function that the model is trying to minimize. The penalty term is proportional to the magnitude of the coefficients of the model. The purpose of this penalty term is to reduce the magnitude of the coefficients and thus reduce the complexity of the model."
      ],
      "metadata": {
        "id": "lPwSKe7b6Cci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- L2 regularization is more commonly used than L1 regularization because it has a more desirable property known as “ridge regression”. Ridge regression can be used to reduce the variance of the coefficients in the model, which can help prevent overfitting."
      ],
      "metadata": {
        "id": "9QriA64I6EnK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DQonZ7j0-BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
        "model?**"
      ],
      "metadata": {
        "id": "C5xmAdNm0-Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- logistic regression, we can use the ROC curve to evaluate the performance of our model. \n",
        "- When we create a ROC curve, we plot pairs of the true positive rate vs. the false positive rate for every possible decision threshold of a logistic regression model.\n",
        "\n",
        "How to Interpret a ROC Curve\n",
        "- The more that the ROC curve hugs the top left corner of the plot, the better the model does at classifying the data into categories."
      ],
      "metadata": {
        "id": "GSG2e-RV6hzT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2lV-ixn1CJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhFa_9Tu1Cvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
        "techniques help improve the model's performance?**"
      ],
      "metadata": {
        "id": "MCnTckdF1C9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature Engineering is an important component of a data science model development pipeline. ‘More data leads to a better machine learning model’, holds true for the number of instances but not for the number of features.\n",
        "-  A raw dataset contains a lot of redundant features that may impact the performance of the model. Feature Selection is a feature engineering component that involves the removal of irrelevant features and picks the best set of features to train a robust machine learning model.\n",
        "- Feature Selection methods reduce the dimensionality of the data and avoid the problem of the curse of dimensionality.\n",
        "**7 Feature Selection Techniques in Machine Learning** :7 ways to select the best features to train a robust machine learning model.\n",
        "\n",
        "- **. Domain Knowledge:** For example, for a car price prediction problem, some features like manufacture year, fancy license number are key factors deciding the price of the car.\n",
        "-  **Missing Values**\n",
        "- **Correlation with the target class label**\n",
        "- **Correlation between the features**\n",
        "- **Principal Component Analysis (PCA)**\n",
        "- **Forward Feature Selection:**: Forward or Backward feature selection techniques are used to find the subset of best-performing features for the machine learning model. For a given dataset if there are n features, the features are selected based on the inference of previous results. The forward feature selection techniques follow:\n",
        "\n",
        "       - Train the model using each of the n features, and evaluate the performance.\n",
        "       - The feature or set of features with the best performance is/are finalized.\n",
        "       - Repeat steps 1 and 2 until you get the desired number of features."
      ],
      "metadata": {
        "id": "K_MnkbJY8r-F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gVj2W3zD1Hqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
        "with class imbalance?**"
      ],
      "metadata": {
        "id": "H2UjxsZt1H6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are several strategies for dealing with class imbalance in logistic regression. One of the popular techniques is up-sampling (e.g. SMOTE) in which more similar data points are added to minority class to make class distribution equal. \n",
        "- Dealing with imbalanced datasets entails strategies such as improving classification algorithms or balancing classes in the training data (data preprocessing) before providing the data as input to the machine learning algorithm. The later technique is preferred as it has wider application.\n",
        "\n",
        "- imbalance techniques for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling)."
      ],
      "metadata": {
        "id": "tsgFnn5e_DNd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sag4VM6-1N5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMUL8fQL1Olt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
        "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
        "among the independent variables?**"
      ],
      "metadata": {
        "id": "VHyqdvsp1O0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Logistic regression is a popular statistical method used for analyzing data. However, it has some limitations and challenges that you should be aware of. Some of the common issues and challenges that may arise when implementing logistic regression are:"
      ],
      "metadata": {
        "id": "TI7Hlqlb_uMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   -  - Overfitting: If the number of observations is less than the number of features, logistic regression may lead to overfitting. This can be addressed by using regularization techniques such as L1 or L2 regularization.\n",
        "\n",
        "   - Linearity assumption: The assumption of linearity between the dependent variable and the independent variables limits the capacity of logistic regression. This can be addressed by using non-linear models such as decision trees or neural networks.\n",
        "   - Multicollinearity: Multicollinearity among the independent variables can lead to unstable estimates of the logistic regression coefficients. This can be addressed by removing one or more of the correlated variables or by using regularization techniques."
      ],
      "metadata": {
        "id": "wRo0oXmY_0p8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZAhReGT1UAF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}